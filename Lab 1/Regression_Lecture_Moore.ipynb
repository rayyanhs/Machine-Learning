{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression on House Pricing Dataset\n",
    "We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "Link to the dataset.\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict.\n",
    "\n",
    "A version of the dataset is in the ZIP file where you got this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put here your ``numero di matricola''\n",
    "ID_number = 1 # COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get in-line plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Load the data from a .csv file\n",
    "\n",
    "**TO DO: insert your ID number (matricola)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(ID_number)\n",
    "\n",
    "filename = \"kc_house_data.csv\"\n",
    "\n",
    "#load the data\n",
    "df = pd.read_csv(filename, sep = ',')\n",
    "\n",
    "#let's print out the data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick overview of data\n",
    "\n",
    "Now let's clean the data and inspect it using the method describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "df.describe()\n",
    "#for more interesting visualization: use Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input and output data. We want to predict the price by using features other than id as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in training and test set\n",
    "\n",
    "Given $m$ total data, keep $m_t$ data as training data, and $m_{test}:=m - m_t$ for test data. For instance one can take $m_t=\\frac{3}{4}m $ of the data as training, and $m_{test}=\\frac{m}{4}$ as testing. Let us define\n",
    "- $S_{t}$ the training data set\n",
    "- $S_{test}$ the testing data set\n",
    "\n",
    "\n",
    "The reason for this splitting is as follows:\n",
    "\n",
    "TRAINING DATA: The training data are used to compute the empirical loss\n",
    "$$\n",
    "L_S(h) = \\frac{1}{m_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "which is used to get $h_S$ in a given model class ${\\cal H}$.\n",
    "i.e. \n",
    "$$\n",
    "h_S = {\\rm arg\\; min}_{h \\in {\\cal H}} \\, L_S(h)\n",
    "$$\n",
    "\n",
    "TESTING DATA: Last, the test data set can be used to estimate the performance of the chosen hypothesis $h_{S}$ using:\n",
    "\n",
    "$$\n",
    "L_{\\cal D}(h_S)  \\simeq \\frac{1}{m_{test}} \\sum_{ z_i \\in S_{test}} \\ell(h_{S},z_i)\n",
    "$$\n",
    "\n",
    "**TO DO: split the data in training and test sets (suggestion: use $m_t=\\left\\lfloor\\frac{3}{4}m\\right\\rfloor $, $m_{test} = m-m_t$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's consider only the values in the DataFrame\n",
    "Data = df.values\n",
    "\n",
    "# m = number of input samples\n",
    "m = Data.shape[0]\n",
    "\n",
    "print(\"Total number of samples: \", m)\n",
    "\n",
    "#size of training dataset\n",
    "size_training = # COMPLETE\n",
    "\n",
    "print(\"Number of samples in training data: \", size_training)\n",
    "\n",
    "#shuffle the data (to make sure we get a random split)\n",
    "\n",
    "# COMPLETE\n",
    "\n",
    "#divide data into matrix X of features and target vector Y \n",
    "\n",
    "Y = # COMPLETE\n",
    "X = # COMPLETE\n",
    "\n",
    "#training data\n",
    "\n",
    "X_training = # COMPLETE\n",
    "Y_training = # COMPLETE\n",
    "print(\"Training input data size: \", X_training.shape)\n",
    "print(\"Training output data size: \", Y_training.shape)\n",
    "\n",
    "#test data, to be used to estimate the true loss of the final model\n",
    "X_test = # COMPLETE\n",
    "Y_test = # COMPLETE\n",
    "print(\"Test input data size: \", X_test.shape)\n",
    "print(\"Test output data size: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "It is common practice in Statistics and Machine Learning to scale the data (= each variable) so that it is centered (zero mean) and has standard deviation equal to $1$. This helps in terms of numerical conditioning of the (inverse) problems of learning the model (the coefficients of the linear regression in this case), as well as to give the same scale to all the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data: standardize the training feature matrix\n",
    "from sklearn import preprocessing\n",
    "scaler = # COMPLETE\n",
    "X_training_scaled = # COMPLETE\n",
    "print(\"Mean of the training input data:\", X_training_scaled.mean(axis=0))\n",
    "print(\"Std of the training input data:\",X_training_scaled.std(axis=0))\n",
    "\n",
    "# now we scale the test feature matrix using the same transformation used\n",
    "# for the training dataset, since the weights of the model will be learned\n",
    "# data scaled according to such transformation\n",
    "\n",
    "X_test_scaled = # COMPLETE\n",
    "print(\"Mean of the test input data:\", X_test_scaled.mean(axis=0))\n",
    "print(\"Std of the test input data:\", X_test_scaled.std(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training \n",
    "\n",
    "The model is trained minimizing the empirical error\n",
    "$$\n",
    "L_S(h) := \\frac{1}{N_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "When the loss function is the quadratic loss\n",
    "$$\n",
    "\\ell(h,z) := (y - h(x))^2\n",
    "$$\n",
    "we define  the Residual Sum of Squares (RSS) as\n",
    "$$\n",
    "RSS(h):= \\sum_{z_i \\in S_{t}} \\ell(h,z_i) = \\sum_{z_i \\in S_{t}} (y_i - h(x_i))^2\n",
    "$$ so that the training error becomes\n",
    "$$\n",
    "L_S(h) = \\frac{RSS(h)}{m_t}\n",
    "$$\n",
    "\n",
    "We recal that, for linear models we have $h(x) = <w,x>$ and the Empirical error $L_S(h)$ can be written\n",
    "in terms of the vector of parameters $w$ in the form\n",
    "$$\n",
    "L_S(w) = \\frac{1}{m_t} \\|Y - X w\\|^2\n",
    "$$\n",
    "where $Y$ and $X$ are the matrices whose $i-$th row are, respectively, the output data $y_i$ and the input vectors $x_i^\\top$.\n",
    "\n",
    "The least squares solution is given by the expression\n",
    "$$\n",
    "\\hat w = {\\rm arg\\;min}_w L_S(w) = (X^\\top X)^{-1} X^\\top Y\n",
    "$$\n",
    "When the matrix $X$ is not invertible (or even when it is invertible), the solution can be computed using the Moore-Penrose pseudonverse $(X^\\top X)^{\\dagger}$ of $(X^\\top X)$\n",
    "$$\n",
    "\\hat w = (X^\\top X)^{\\dagger} X^\\top Y\n",
    "$$\n",
    "The Moore-Penrose pseudoinverse $A^\\dagger$ of a matrix $A \\in \\mathbb{R}^{m\\times n}$ can be expressed in terms of the Singular Value Decomposition (SVD) as follows:\n",
    "let $A\\in \\mathbb{R}^{m\\times n}$ be of rank $r\\leq {\\rm min}(n,m)$ and let  \n",
    "$$\n",
    " A = USV^\\top\n",
    " $$\n",
    " be the singular value decomposition of  $A$ where  \n",
    " $$\n",
    " S = {\\rm diag}\\{s_1,s_2,..,s_r\\}\n",
    " $$\n",
    " Then \n",
    " $$\n",
    " A^\\dagger =V S^{-1} U^\\top \n",
    " $$\n",
    " \n",
    " In practice some of the singular values may be very small (e.g. $<1e-10$). Therefore it makes sense to  first approximate the matrix $A$ truncating the SVD and then using the pseudoinverse formula.\n",
    " \n",
    " More specifically, let us postulate that, given a threshold $T_h$ (e.g $=1e-12$), we have $\\sigma_i<T_h$, for $i=\\hat r + 1,..,r$. Then we can approximate (by SVD truncation) $A$ using:\n",
    " \n",
    " $$A = USV^\\top =U \\,{\\rm diag}\\{s_1,s_2,..,s_r\\}\\, V^\\top \\simeq \\hat A_r = U\\,{\\rm diag}\\{s_1,s_2,..,s_{\\hat r}, 0,..,0\\}\\,V^\\top\n",
    " $$\n",
    " So that \n",
    " $$\n",
    " A^\\dagger \\simeq \\hat A_r^\\dagger:= V \\,{\\rm diag}\\{1/s_1,1/s_2,..,1/s_{\\hat r}, 0,..,0\\}\\, U^\\top\n",
    " $$\n",
    "  \n",
    " In numpy, the Moore-Penrose pseudo-inverse of a matrix can be computed using the method numpy.linalg.pinv(...), which takes among its parameters the threshold for truncating the singular values to 0.\n",
    "  \n",
    " **TO DO: compute the linear regression coefficients according to the description above (using numpy.linalg.pinv(...) )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute linear regression coefficients for training data\n",
    "\n",
    "#number of samples in the training set\n",
    "m_training = X_training_scaled.shape[0]\n",
    "\n",
    "#number of samples in the test set\n",
    "m_test = X_test_scaled.shape[0]\n",
    "\n",
    "# add a 1 at the beginning of each sample for training, and testing\n",
    "# the numpy function hstack is useful for such operation\n",
    "X_training_prime = # COMPLETE\n",
    "\n",
    "X_test_prime = # COMPLETE\n",
    "\n",
    "# set precision under which singular values are considered as zeros\n",
    "prec = 1e-10  \n",
    "\n",
    "# compute Moore-Penrose pseudoinverse of the matrix you need to compute \n",
    "# the weights of the model\n",
    "A_inv = # COMPLETE\n",
    "\n",
    "# now compute the weights and print them\n",
    "w_hand = # COMPLETE\n",
    "\n",
    "print(\"LS coefficients by hand:\", w_hand)\n",
    "\n",
    "# compute Residual Sums of Squares by hand\n",
    "RSStr_hand = # COMPLETE\n",
    "\n",
    "# print the RSS\n",
    "print(\"RSS by hand:\",  RSStr_hand)\n",
    "\n",
    "# print the empirical risk\n",
    "print(\"Empirical risk by hand:\", RSStr_hand/m_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prediction \n",
    "\n",
    "Compute the output predictions on both training and test set and compute the Residual Sum of Squares (RSS) defined above, the Empirical Loss and the quantity $R^2$ where\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{z_i \\in S_t} (\\hat y_i - \\bar y)^2}{\\sum_{z_i \\in S_t} (y_i - \\bar y)^2} \\quad \\quad \\bar y = \\frac{1}{m_t} \\sum_{z_i \\in S_t} y_i\n",
    "$$\n",
    "is the so-called \"Coefficient of determination\" (COD).\n",
    "\n",
    "**TO DO Compute these quantities on  training and test data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute predictions on training and test\n",
    "prediction_training = # COMPLETE\n",
    "prediction_test = # COMPLETE\n",
    "\n",
    "#what about the RSS and empirical loss for points in the test data?\n",
    "RSS_test = # COMPLETE\n",
    "\n",
    "print(\"RSS on test data:\",  RSS_test)\n",
    "print(\"Generalization error estimated on test data (i.e., empirical loss on test data):\", RSS_test/m_test)\n",
    "\n",
    "#another measure of how good our linear fit is given by the following (that is R^2)\n",
    "measure_training = # COMPLETE\n",
    "measure_test = # COMPLETE\n",
    "\n",
    "print(\"Measure on Training Data (R^2):\", measure_training)\n",
    "print(\"Measure on Test Data(R^2):\", measure_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and plot:\n",
    "\n",
    "\n",
    "### (1) output predictions on training  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on Training data \n",
    "plt.figure()\n",
    "\n",
    "#the following is just for nice plotting, not required: it sorts the predictions by value so that they fall on\n",
    "# a line and it's easier to spot the differences\n",
    "sorting_permutation = sorted(range(len(prediction_training[0:m_training])), key=lambda k: prediction_training[0:m_training][k])\n",
    "plt.plot(Y_training[sorting_permutation], 'ko', alpha=0.5)\n",
    "plt.plot(prediction_training[sorting_permutation], 'rx')\n",
    "\n",
    "plt.xlabel('Input (index of instance)')\n",
    "plt.ylabel('Predicted Output')\n",
    "plt.title('Predictions on Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) output predictions on test  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on test data \n",
    "plt.figure()\n",
    "\n",
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least-Squares using scikit-learn\n",
    "\n",
    "A fast way to compute the LS estimate is through sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the ``ones'' column in the features matrix (sklearn inserts it automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_OLS = X_training_scaled[:,1:]\n",
    "X_test_OLS = X_test_scaled[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "LinReg = linear_model.LinearRegression()  # build the object LinearRegression\n",
    "LinReg.fit(X_training_OLS, Y_training)  # estimate the LS coefficients\n",
    "print(\"Intercept:\", LinReg.intercept_)\n",
    "print(\"Least-Squares Coefficients:\", LinReg.coef_)\n",
    "prediction_training = LinReg.predict(X_training_OLS)  # predict output values on training set\n",
    "prediction_test = LinReg.predict(X_test_OLS)  # predict output values on test set\n",
    "print(\"Measure on training data:\", LinReg.score(X_training_OLS, Y_training))\n",
    "print(\"Measure on test data:\", LinReg.score(X_test_OLS, Y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
